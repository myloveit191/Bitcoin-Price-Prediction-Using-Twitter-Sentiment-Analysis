#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bitcoin Price Prediction Pipeline
D·ª± ƒëo√°n gi√° Bitcoin th√¥ng qua ph√¢n t√≠ch c·∫£m x√∫c Twitter

Author: Your Name
Date: 2025-09-19
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from configparser import ConfigParser
import warnings
warnings.filterwarnings('ignore')
from preprocessing import preprocess_twitter_data

# Import c√°c module local (s·∫Ω implement sau)
# from sentiment_analysis import analyze_sentiment
# from feature_engineering import create_features
# from model_training import train_models
# from evaluation import evaluate_models

def load_config():
    """Load configuration t·ª´ config.ini"""
    print("üîß Loading configuration...")
    config = ConfigParser()
    try:
        with open('config.ini', 'r', encoding='utf-8') as f:
            config.read_file(f)
        print("   ‚úÖ Config loaded successfully")
        return config
    except Exception as e:
        print(f"   ‚ùå Error loading config: {e}")
        sys.exit(1)

def load_data(config):
    """Load d·ªØ li·ªáu Bitcoin v√† Twitter"""
    print("\nüìä Loading datasets...")
    
    # Load Twitter data
    twitter_file = config.get('File', 'filename', fallback='bitcoin_tweets_1month_20250919_161545.csv')
    print(f"   üì± Loading Twitter data: {twitter_file}")
    tweets_path = os.path.join('data', 'tweets', twitter_file)
    if os.path.exists(tweets_path):
        twitter_df = pd.read_csv(tweets_path, encoding='utf-8')
        print(f"   ‚úÖ Twitter data loaded: {len(twitter_df)} tweets")
        print(f"      - Columns: {list(twitter_df.columns)}")
        print(f"      - Date range: {twitter_df['date'].min()} to {twitter_df['date'].max()}" if 'date' in twitter_df.columns else "")
    else:
        print(f"   ‚ùå Twitter file not found: {tweets_path}")
        sys.exit(1)
    
    # Load Bitcoin data (find latest file)
    bitcoin_files = [f for f in os.listdir('.') if f.startswith('bitcoin_price_data_') and f.endswith('.csv')]
    if bitcoin_files:
        bitcoin_file = sorted(bitcoin_files)[-1]  # Latest file
        print(f"   ‚Çø Loading Bitcoin data: {bitcoin_file}")
        
        bitcoin_df = pd.read_csv(bitcoin_file, encoding='utf-8')
        print(f"   ‚úÖ Bitcoin data loaded: {len(bitcoin_df)} records")
        print(f"      - Columns: {list(bitcoin_df.columns)}")
        print(f"      - Date range: {bitcoin_df['date'].min()} to {bitcoin_df['date'].max()}" if 'date' in bitcoin_df.columns else "")
    else:
        print("   ‚ùå No Bitcoin data files found")
        sys.exit(1)
    
    return twitter_df, bitcoin_df

def data_preprocessing_twitter(twitter_df):
    """Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"""
    print("\nüßπ Data Preprocessing...")
    
    # Twitter data preprocessing using advanced preprocessing pipeline
    print("   üì± Processing Twitter data...")
    if not twitter_df.empty:
        print(f"      - Original tweets: {len(twitter_df)}")
        
        # Import and use the preprocessing pipeline
        try:
            
            
            # Configure preprocessing pipeline
            preprocessing_config = {
                'text_cleaning': {
                    'decode_html': True,
                    'convert_emojis': True,
                    'remove_urls': True,
                    'handle_mentions': True,
                    'handle_hashtags': True,
                    'remove_rt': True,
                    'remove_noise': True,
                    'normalize': True,
                    'keep_hashtags': True,
                    'keep_mentions': False
                },
                'tokenization': {
                    'preserve_crypto': True,
                    'remove_stopwords': True,
                    'remove_mentions': True,
                    'remove_hashtags': False,
                    'remove_urls': True,
                    'remove_numbers': True,
                    'remove_punctuation': True,
                    'min_length': 2,
                    'max_length': 50
                },
                'embeddings': {
                    'embedding_types': ['tfidf', 'word2vec', 'fasttext'],
                    'save_embeddings': True,
                    'save_dir': 'data/embeddings',
                    'tfidf_params': {
                        'max_features': 5000,
                        'min_df': 2,
                        'max_df': 0.95,
                        'ngram_range': (1, 2)
                    }
                },
                'output': {
                    'save_intermediate': True,
                    'save_final': True,
                    'output_dir': 'data/preprocessed'
                }
            }
            
            # Run preprocessing pipeline (without sentence segmentation)
            preprocessing_results = preprocess_twitter_data(
                twitter_df, 
                text_column='text',
                config=preprocessing_config,
                steps=['cleaning', 'tokenization', 'embeddings']  # Removed 'segmentation'
            )
            
            # Extract processed data
            if 'tokenized_data' in preprocessing_results.get('data', {}):
                processed_twitter = preprocessing_results['data']['tokenized_data']
                print(f"      ‚úÖ Advanced preprocessing completed: {len(processed_twitter)} processed texts")
                print(f"      - Generated embeddings: {list(preprocessing_results.get('data', {}).get('embeddings', {}).keys())}")
                print(f"      - Vocabulary size: {preprocessing_results.get('statistics', {}).get('tokenization', {}).get('vocabulary_size', 'N/A')}")
                
                # Store preprocessing results for later use
                processed_twitter.preprocessing_results = preprocessing_results
            else:
                print("      ‚ö†Ô∏è Preprocessing pipeline failed, using original data")
                processed_twitter = twitter_df.copy()
                
        except ImportError as e:
            print(f"      ‚ö†Ô∏è Could not import preprocessing pipeline: {e}")
            print("      - Using basic preprocessing fallback")
            processed_twitter = twitter_df.copy()
        except Exception as e:
            print(f"      ‚ö†Ô∏è Preprocessing pipeline error: {e}")
            print("      - Using original data")
            processed_twitter = twitter_df.copy()
    else:
        sys.exit(1)
    
    
    return processed_twitter

def sentiment_analysis(twitter_df):
    """Ph√¢n t√≠ch c·∫£m x√∫c Twitter data"""
    print("\nüé≠ Sentiment Analysis...")
    
    if twitter_df.empty:
        print("   ‚ùå No Twitter data for sentiment analysis")
        return pd.DataFrame()
    
    print(f"   üìä Analyzing sentiment for {len(twitter_df)} tweets...")
    
    # TODO: Implement sentiment analysis
    print("   üîç [TODO] TextBlob sentiment analysis")
    print("      - [TODO] Calculate polarity (-1 to 1)")
    print("      - [TODO] Calculate subjectivity (0 to 1)")
    
    print("   üîç [TODO] VADER sentiment analysis")
    print("      - [TODO] Calculate compound score")
    print("      - [TODO] Calculate pos/neg/neu scores")
    
    print("   üìà [TODO] Aggregate sentiment by time periods")
    print("      - [TODO] Hourly sentiment averages")
    print("      - [TODO] Daily sentiment trends")
    print("      - [TODO] Sentiment volatility metrics")
    
    # Placeholder result
    sentiment_df = twitter_df.copy()
    sentiment_df['sentiment_polarity'] = np.random.uniform(-1, 1, len(sentiment_df))  # Mock data
    sentiment_df['sentiment_subjectivity'] = np.random.uniform(0, 1, len(sentiment_df))  # Mock data
    
    print(f"   ‚úÖ Sentiment analysis completed")
    print(f"      - Average polarity: {sentiment_df['sentiment_polarity'].mean():.3f}")
    print(f"      - Average subjectivity: {sentiment_df['sentiment_subjectivity'].mean():.3f}")
    
    return sentiment_df

def data_synchronization(twitter_df, bitcoin_df):
    """ƒê·ªìng b·ªô d·ªØ li·ªáu Twitter v√† Bitcoin theo th·ªùi gian"""
    print("\nüîÑ Data Synchronization...")
    
    if twitter_df.empty or bitcoin_df.empty:
        print("   ‚ùå Missing data for synchronization")
        return pd.DataFrame()
    
    print("   ‚è∞ Synchronizing Twitter and Bitcoin data...")
    
    # TODO: Implement data synchronization
    print("    [TODO] Parse and standardize timestamps")
    print("   üïê [TODO] Aggregate Twitter sentiment by hour")
    print("   üìä [TODO] Merge with Bitcoin hourly data")
    print("    [TODO] Handle missing time periods")
    print("   üìà [TODO] Create lagged features")
    
    # Placeholder result
    print("   ‚úÖ Data synchronization completed")
    print(f"      - [TODO] Synchronized records: XXX")
    print(f"      - [TODO] Time range: YYYY-MM-DD to YYYY-MM-DD")
    
    return pd.DataFrame()  # Placeholder

def feature_engineering(merged_df):
    """T·∫°o features cho machine learning"""
    print("\n‚öôÔ∏è Feature Engineering...")
    
    if merged_df.empty:
        print("   ‚ùå No merged data for feature engineering")
        return pd.DataFrame(), []
    
    print("   üîß Creating features...")
    
    # TODO: Implement feature engineering
    print("   üìà [TODO] Technical indicators")
    print("      - [TODO] Moving averages (MA5, MA10, MA20)")
    print("      - [TODO] RSI (Relative Strength Index)")
    print("      - [TODO] MACD indicators")
    print("      - [TODO] Bollinger Bands")
    
    print("   üé≠ [TODO] Sentiment features")
    print("      - [TODO] Sentiment moving averages")
    print("      - [TODO] Sentiment volatility")
    print("      - [TODO] Sentiment momentum")
    
    print("   üí∞ [TODO] Price features")
    print("      - [TODO] Price returns")
    print("      - [TODO] Log returns")
    print("      - [TODO] Volatility measures")
    
    print("    [TODO] Temporal features")
    print("      - [TODO] Hour of day")
    print("      - [TODO] Day of week")
    print("      - [TODO] Time-based lags")
    
    feature_names = ['feature_1', 'feature_2', 'feature_3']  # Placeholder
    print(f"   ‚úÖ Feature engineering completed")
    print(f"      - [TODO] Total features created: {len(feature_names)}")
    
    return pd.DataFrame(), feature_names  # Placeholder

def model_training(X, y, feature_names):
    """Training c√°c m√¥ h√¨nh machine learning"""
    print("\n Model Training...")
    
    if len(X) == 0 or len(y) == 0:
        print("   ‚ùå No data for model training")
        return {}
    
    print(f"   üìä Training data: {len(X)} samples, {len(feature_names)} features")
    
    # TODO: Implement model training
    print("   üîÑ [TODO] Train/Test split (80/20)")
    print("    [TODO] Feature scaling/normalization")
    
    print("   üß† [TODO] Training models:")
    print("      - [TODO] Random Forest")
    print("      - [TODO] XGBoost")
    print("      - [TODO] LSTM Neural Network")
    print("      - [TODO] Support Vector Machine")
    print("      - [TODO] Linear Regression (baseline)")
    
    print("   ‚öôÔ∏è [TODO] Hyperparameter tuning")
    print("   üìà [TODO] Cross-validation")
    
    models = {}  # Placeholder
    print("   ‚úÖ Model training completed")
    
    return models

def model_evaluation(models, X_test, y_test):
    """ƒê√°nh gi√° hi·ªáu su·∫•t c√°c m√¥ h√¨nh"""
    print("\nüìä Model Evaluation...")
    
    if not models:
        print("   ‚ùå No trained models to evaluate")
        return {}
    
    # TODO: Implement model evaluation
    print("   üìà [TODO] Evaluation metrics:")
    print("      - [TODO] Accuracy")
    print("      - [TODO] Precision/Recall/F1-Score")
    print("      - [TODO] ROC AUC")
    print("      - [TODO] Mean Squared Error")
    print("      - [TODO] Mean Absolute Error")
    
    print("   üìä [TODO] Model comparison")
    print("    [TODO] Feature importance analysis")
    print("   üìà [TODO] Learning curves")
    print("    [TODO] Error analysis")
    
    results = {}  # Placeholder
    print("   ‚úÖ Model evaluation completed")
    
    return results

def generate_predictions(best_model, X_future):
    """T·∫°o d·ª± ƒëo√°n cho t∆∞∆°ng lai"""
    print("\nüîÆ Generating Predictions...")
    
    # TODO: Implement prediction generation
    print("    [TODO] Generate future predictions")
    print("   üìä [TODO] Confidence intervals")
    print("   üìà [TODO] Trend analysis")
    print("   üíæ [TODO] Save predictions to file")
    
    print("   ‚úÖ Predictions generated")

def create_visualizations(twitter_df, bitcoin_df, results):
    """T·∫°o c√°c bi·ªÉu ƒë·ªì v√† visualizations"""
    print("\nüìà Creating Visualizations...")
    
    # TODO: Implement visualizations
    print("   üìä [TODO] Data exploration plots")
    print("      - [TODO] Bitcoin price trends")
    print("      - [TODO] Sentiment distribution")
    print("      - [TODO] Correlation heatmap")
    
    print("   üé≠ [TODO] Sentiment analysis plots")
    print("      - [TODO] Sentiment vs Price correlation")
    print("      - [TODO] Time series sentiment trends")
    
    print("   ü§ñ [TODO] Model performance plots")
    print("      - [TODO] Model comparison charts")
    print("      - [TODO] Feature importance plots")
    print("      - [TODO] Prediction vs Actual")
    
    print("   üíæ [TODO] Save plots to charts/ directory")
    print("   ‚úÖ Visualizations created")

def generate_report(results):
    """T·∫°o b√°o c√°o t·ªïng k·∫øt"""
    print("\nüìù Generating Final Report...")
    
    # TODO: Implement report generation
    print("   üìä [TODO] Summary statistics")
    print("   üéØ [TODO] Model performance summary")
    print("   üìà [TODO] Key insights and findings")
    print("    [TODO] Recommendations")
    print("   üíæ [TODO] Save report to file")
    
    print("   ‚úÖ Report generated")

def main():
    """Main pipeline function"""
    print("üöÄ BITCOIN PRICE PREDICTION PIPELINE")
    print("=" * 60)
    print(" Started at:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("üéØ Goal: Predict Bitcoin price using Twitter sentiment")
    print("=" * 60)
    
    try:
        # 1. Load configuration
        config = load_config()
        
        # 2. Load data
        twitter_df, bitcoin_df = load_data(config)
        
        # 3. Data preprocessing
        processed_twitter = data_preprocessing_twitter(twitter_df)
        
        # 4. Sentiment analysis
        twitter_with_sentiment = sentiment_analysis(processed_twitter)
        
        # 5. Data synchronization
        merged_df = data_synchronization(twitter_with_sentiment, bitcoin_df)
        
        # 6. Feature engineering
        X, feature_names = feature_engineering(merged_df)
        
        # 7. Prepare target variable (placeholder)
        y = []  # TODO: Extract from merged_df
        
        # 8. Model training
        models = model_training(X, y, feature_names)
        
        # 9. Model evaluation
        results = model_evaluation(models, [], [])  # TODO: Pass test data
        
        # 10. Generate predictions
        generate_predictions(None, [])  # TODO: Pass best model and future data
        
        # 11. Create visualizations
        create_visualizations(twitter_df, bitcoin_df, results)
        
        # 12. Generate report
        generate_report(results)
        
        print("\nüéâ PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 60)
        print("üìÖ Finished at:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        print("üìä Check output files and charts/ directory for results")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Pipeline interrupted by user")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå Pipeline failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
